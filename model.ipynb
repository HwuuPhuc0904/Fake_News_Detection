{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "855af5e1-7a60-4141-a14f-827cb4936f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def models(X, y, option = 0, val_size=0.2, random_state=0):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=random_state)\n",
    "\n",
    "    if option == 0:\n",
    "    \n",
    "        params = {\n",
    "        'model' : KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "        \n",
    "        \n",
    "\n",
    "    elif option == 1:\n",
    "        params = {\n",
    "            'model': DecisionTreeClassifier(),\n",
    "            'params' : {\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'splitter': ['best', 'random'],\n",
    "                'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "        'model' : LogisticRegression(),\n",
    "        'params' : {\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'max_iter': [100, 200, 500, 1000],\n",
    "            'class_weight': [None, 'balanced']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    # Report\n",
    "    print(f'Report of train set with {int(len(X_train))} examples: ')\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print('\\nReport of validation set:')\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "    return model\n",
    "\n",
    "def cat_models(X, y, option = 0, val_size=0.2, random_state=0):\n",
    "    pipeline = Pipeline([('count',CountVectorizer()),('tfidf',TfidfTransformer())])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=random_state)\n",
    "    X_train = pipeline.fit_transform(X_train).toarray()\n",
    "    X_val = pipeline.transform(X_val).toarray()\n",
    "\n",
    "\n",
    "    if option == 0:\n",
    "    \n",
    "        params = {\n",
    "        'model' : KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "        \n",
    "        \n",
    "\n",
    "    elif option == 1:\n",
    "        params = {\n",
    "            'model': DecisionTreeClassifier(),\n",
    "            'params' : {\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'splitter': ['best', 'random'],\n",
    "                'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "        'model' : LogisticRegression(),\n",
    "        'params' : {\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'max_iter': [100, 200, 500, 1000],\n",
    "            'class_weight': [None, 'balanced']\n",
    "            }\n",
    "        }\n",
    "        model = find_best_model(params[\"model\"], params[\"params\"], X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    # Report\n",
    "    print(f'Report of train set with {int(len(X_train))} examples: ')\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print('\\nReport of validation set:')\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "    return model, pipeline\n",
    "\n",
    "def predict(model, X, y, name_set='test set'):\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    stop = time.time()\n",
    "\n",
    "    print(f'Time to predict on the {name_set} with {len(X)} examples: {round(stop-start, 4)}s')\n",
    "    print(\"Precision:\", precision_score(y, y_pred))\n",
    "    print(\"Recall:\", recall_score(y, y_pred))\n",
    "    print(\"F1-score:\", f1_score(y, y_pred))\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        print(\"ROC AUC Score:\", roc_auc_score(y, y_prob))\n",
    "    else:\n",
    "        print(\"ROC AUC Score: N/A (model doesn't support probability estimates)\")\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cat_predict(model, pipeline, X, y, name_set='test set'):\n",
    "    X = pipeline.transform(X).toarray()\n",
    "    \n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    stop = time.time()\n",
    "\n",
    "    print(f'Time to predict on the {name_set} with {len(X)} examples: {round(stop-start, 4)}s')\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de6f446-6e3d-4a1b-b005-db8e39878a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad48ac0-d455-45a8-b524-f8125e777474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073,) (1073,)\n",
      "(793,) (793,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('downsample_data.csv', encoding='utf8')\n",
    "data_test = pd.read_csv('test.csv', encoding='utf8')\n",
    "X, y = data.post_message_preproced.values, data.label.values\n",
    "X_test, y_test = data_test.post_message_preproced.values, data_test.label.values\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16a7e2a6-e5eb-4115-b051-94f33fc65a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "\n",
    "def find_best_model(model, param_grid, X_train, y_train):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_param = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    print(\"Best param: \", best_param)\n",
    "    print(\"Best score: \", best_score)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0d6ce5-93db-4184-9202-c5de2036e8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model:\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best param:  {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "Best score:  0.7272065823473411\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       448\n",
      "           1       1.00      1.00      1.00       410\n",
      "\n",
      "    accuracy                           1.00       858\n",
      "   macro avg       1.00      1.00      1.00       858\n",
      "weighted avg       1.00      1.00      1.00       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       114\n",
      "           1       0.80      0.77      0.79       101\n",
      "\n",
      "    accuracy                           0.80       215\n",
      "   macro avg       0.80      0.80      0.80       215\n",
      "weighted avg       0.80      0.80      0.80       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.1994s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.84       663\n",
      "           1       0.37      0.70      0.49       130\n",
      "\n",
      "    accuracy                           0.76       793\n",
      "   macro avg       0.65      0.73      0.66       793\n",
      "weighted avg       0.84      0.76      0.78       793\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Randomforest model:\n",
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "Best param:  {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "Best score:  0.6969604243166054\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       448\n",
      "           1       1.00      1.00      1.00       410\n",
      "\n",
      "    accuracy                           1.00       858\n",
      "   macro avg       1.00      1.00      1.00       858\n",
      "weighted avg       1.00      1.00      1.00       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71       114\n",
      "           1       0.67      0.62      0.65       101\n",
      "\n",
      "    accuracy                           0.68       215\n",
      "   macro avg       0.68      0.68      0.68       215\n",
      "weighted avg       0.68      0.68      0.68       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0193s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.73      0.81       663\n",
      "           1       0.31      0.62      0.41       130\n",
      "\n",
      "    accuracy                           0.71       793\n",
      "   macro avg       0.61      0.67      0.61       793\n",
      "weighted avg       0.81      0.71      0.74       793\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Logistic model:\n",
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n",
      "Best param:  {'C': 10, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best score:  0.7633754929960561\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       448\n",
      "           1       1.00      1.00      1.00       410\n",
      "\n",
      "    accuracy                           1.00       858\n",
      "   macro avg       1.00      1.00      1.00       858\n",
      "weighted avg       1.00      1.00      1.00       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       114\n",
      "           1       0.85      0.82      0.83       101\n",
      "\n",
      "    accuracy                           0.85       215\n",
      "   macro avg       0.85      0.85      0.85       215\n",
      "weighted avg       0.85      0.85      0.85       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0156s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85       663\n",
      "           1       0.40      0.77      0.53       130\n",
      "\n",
      "    accuracy                           0.77       793\n",
      "   macro avg       0.67      0.77      0.69       793\n",
      "weighted avg       0.86      0.77      0.80       793\n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Stack model:\n",
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n",
      "Best param:  {'C': 10, 'class_weight': 'balanced', 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best score:  0.7633754929960561\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       448\n",
      "           1       1.00      1.00      1.00       410\n",
      "\n",
      "    accuracy                           1.00       858\n",
      "   macro avg       1.00      1.00      1.00       858\n",
      "weighted avg       1.00      1.00      1.00       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       114\n",
      "           1       0.85      0.82      0.83       101\n",
      "\n",
      "    accuracy                           0.85       215\n",
      "   macro avg       0.85      0.85      0.85       215\n",
      "weighted avg       0.85      0.85      0.85       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0157s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85       663\n",
      "           1       0.40      0.77      0.53       130\n",
      "\n",
      "    accuracy                           0.77       793\n",
      "   macro avg       0.67      0.77      0.69       793\n",
      "weighted avg       0.86      0.77      0.80       793\n",
      "\n",
      "**********************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "options = [0,1,2]\n",
    "names = ['KNN','Randomforest', 'Logistic']\n",
    "\n",
    "for (option, name) in zip(options, names):\n",
    "    print(f'{name} model:')\n",
    "    model, pipeline = cat_models(X, y, option=option)\n",
    "    _ = cat_predict(model=model, pipeline=pipeline, X=X_test, y=y_test)\n",
    "    print('*'*130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4f84476-38bf-4b99-a095-a7741da75409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073, 4) (1073,)\n",
      "(793, 4) (793,)\n",
      "KNN model:\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best param:  {'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Best score:  0.6165170678634571\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73       448\n",
      "           1       0.72      0.62      0.66       410\n",
      "\n",
      "    accuracy                           0.70       858\n",
      "   macro avg       0.71      0.70      0.70       858\n",
      "weighted avg       0.70      0.70      0.70       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65       114\n",
      "           1       0.60      0.52      0.56       101\n",
      "\n",
      "    accuracy                           0.61       215\n",
      "   macro avg       0.61      0.60      0.60       215\n",
      "weighted avg       0.61      0.61      0.61       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0629s\n",
      "Precision: 0.28044280442804426\n",
      "Recall: 0.5846153846153846\n",
      "F1-score: 0.3790523690773067\n",
      "ROC AUC Score: 0.6950864369416404\n",
      "****************************************************************************************************\n",
      "DecisionTree model:\n",
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n",
      "Best param:  {'criterion': 'gini', 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'random'}\n",
      "Best score:  0.6853053175574594\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78       448\n",
      "           1       0.78      0.70      0.74       410\n",
      "\n",
      "    accuracy                           0.76       858\n",
      "   macro avg       0.77      0.76      0.76       858\n",
      "weighted avg       0.77      0.76      0.76       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72       114\n",
      "           1       0.69      0.60      0.65       101\n",
      "\n",
      "    accuracy                           0.69       215\n",
      "   macro avg       0.69      0.68      0.68       215\n",
      "weighted avg       0.69      0.69      0.69       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0s\n",
      "Precision: 0.3374485596707819\n",
      "Recall: 0.6307692307692307\n",
      "F1-score: 0.43967828418230565\n",
      "ROC AUC Score: 0.7212437637777005\n",
      "****************************************************************************************************\n",
      "Logistic model:\n",
      "Fitting 5 folds for each of 600 candidates, totalling 3000 fits\n",
      "Best param:  {'C': 0.01, 'class_weight': None, 'max_iter': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best score:  0.5758057935536515\n",
      "Report of train set with 858 examples: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.96      0.71       448\n",
      "           1       0.80      0.20      0.32       410\n",
      "\n",
      "    accuracy                           0.59       858\n",
      "   macro avg       0.69      0.58      0.52       858\n",
      "weighted avg       0.68      0.59      0.52       858\n",
      "\n",
      "\n",
      "Report of validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.90      0.68       114\n",
      "           1       0.61      0.17      0.26       101\n",
      "\n",
      "    accuracy                           0.56       215\n",
      "   macro avg       0.58      0.54      0.47       215\n",
      "weighted avg       0.58      0.56      0.49       215\n",
      "\n",
      "Time to predict on the test set with 793 examples: 0.0s\n",
      "Precision: 0.43661971830985913\n",
      "Recall: 0.23846153846153847\n",
      "F1-score: 0.30845771144278605\n",
      "ROC AUC Score: 0.596484510964149\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "cols = ['user_name_labelEncoder', 'num_like_post', 'num_comment_post', 'num_share_post']\n",
    "X, y = data[cols].values, data.label.values\n",
    "X_test, y_test = data_test[cols].values, data_test.label.values\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "options = [0,1,2]\n",
    "names = ['KNN', 'DecisionTree', 'Logistic']\n",
    "\n",
    "for (option, name) in zip(options, names):\n",
    "    print(f'{name} model:')\n",
    "    model = models(X, y, option=option)\n",
    "    _ =  predict(model=model, X=X_test, y=y_test)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48367864-7508-4c68-aeda-5862a24ff9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
